{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# NLP 2025\n",
    "# Lab 1: Tokenization\n",
    "\n",
    "Tokenization is a fundamental step in **Natural Language Processing (NLP)** 🧠💬 that transforms raw text into structured data for computational models. In this lab, you will explore different **tokenization techniques** 📝, preprocess text data 🔍, and implement **tokenization pipelines** using popular NLP libraries 🏗️.  \n",
    "\n",
    "You will also gain **hands-on experience** with **Hugging Face Datasets 🤗📚**, while assessing the impact of tokenization choices on downstream NLP tasks. \n",
    "\n",
    "By the end of this lab, you will have a **strong foundation** in tokenization techniques and be able to apply them effectively in **real-world NLP applications** 🌍.  \n",
    "\n",
    "---\n",
    "\n",
    "### 🎯 **Learning Goals**  \n",
    "\n",
    "By the end of this lab, you should be able to:  \n",
    "\n",
    "✅ **Understand the role of tokenization in NLP** 🧠💡  \n",
    "✅ **Explain why tokenization is important** and how it affects text processing 📖🔍  \n",
    "✅ **Implement different tokenization techniques** – Apply **word** 📝, **subword** 🔢, and **character-level** 🔠 tokenization using built-in libraries.  \n",
    "✅ **Use Hugging Face Datasets** 🤗📊 – Load and preprocess text datasets efficiently.  \n",
    "✅ **Evaluate tokenization impact** 📉🔎 – Analyze how different tokenization methods influence model performance.  \n",
    "✅ **Identify challenges in tokenization** ❗🔍 – Recognize issues like **out-of-vocabulary (OOV) words**, **ambiguity**, and **multilingual tokenization** 🌍.  \n",
    "\n",
    "### Score breakdown\n",
    "\n",
    "| Exercise            | Points |\n",
    "|---------------------|--------|\n",
    "| [Exercise 1](#e1)   | 5      |\n",
    "| [Exercise 2](#e2)   | 6      |\n",
    "| [Exercise 3](#e3)   | 5      |\n",
    "| [Exercise 4](#e4)   | 12     |\n",
    "| [Exercise 5](#e5)   | 5      |\n",
    "| [Exercise 6](#e6)   | 22     |\n",
    "| [Exercise 7](#e7)   | 5      |\n",
    "| [Exercise 8](#e8)   | 5      |\n",
    "| [Exercise 9](#e9)   | 10     |\n",
    "| [Exercise 10](#e10) | 25     |\n",
    "| Total               | 100    |\n",
    "\n",
    "This score will be scaled down to 0.5 and that will be your final lab score.\n",
    "\n",
    "### 📌 **Instructions for Delivery** (📅 **Deadline: 11/Apr 18:00**, 🎭 *wildcards possible*)  \n",
    "\n",
    "✅ **Submission Requirements**  \n",
    "+ 📄 You need to submit a **PDF of your report** (use the templates provided in **LaTeX** 🖋️ (*preferred*) or **Word** 📑) and a **copy of your notebook** 📓 with the code.  \n",
    "+ ⚡ Make sure that **all cells are executed properly** ⚙️ and that **all figures/results/plots** 📊 you include in the report are also visible in your **executed notebook**.  \n",
    "\n",
    "✅ **Collaboration & Integrity**  \n",
    "+ 🗣️ While you may **discuss** the lab with others, you must **write your solutions with your group only**. If you **discuss specific tasks** with others, please **include their names** in the appendix of the report.  \n",
    "+ 📜 **Honor Code applies** to this lab. For more details, check **Syllabus §7.2** ⚖️.  \n",
    "+ 📢 **Mandatory Disclosure**:  \n",
    "   - Any **websites** 🌐 (e.g., **Stack Overflow** 💡) or **other resources** used must be **listed and disclosed**.  \n",
    "   - Any **GenAI tools** 🤖 (e.g., **ChatGPT**) used must be **explicitly mentioned**.  \n",
    "   - 🚨 **Failure to disclose these resources is a violation of academic integrity**. See **Syllabus §7.3** for details.   "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets~=3.2.0\r\n",
      "  Obtaining dependency information for datasets~=3.2.0 from https://files.pythonhosted.org/packages/d7/84/0df6c5981f5fc722381662ff8cfbdf8aad64bec875f75d80b55bfef394ce/datasets-3.2.0-py3-none-any.whl.metadata\r\n",
      "  Downloading datasets-3.2.0-py3-none-any.whl.metadata (20 kB)\r\n",
      "Requirement already satisfied: filelock in /Users/vd/anaconda3/lib/python3.11/site-packages (from datasets~=3.2.0) (3.9.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/vd/anaconda3/lib/python3.11/site-packages (from datasets~=3.2.0) (1.24.3)\r\n",
      "Collecting pyarrow>=15.0.0 (from datasets~=3.2.0)\r\n",
      "  Obtaining dependency information for pyarrow>=15.0.0 from https://files.pythonhosted.org/packages/a0/55/f1a8d838ec07fe3ca53edbe76f782df7b9aafd4417080eebf0b42aab0c52/pyarrow-19.0.1-cp311-cp311-macosx_12_0_arm64.whl.metadata\r\n",
      "  Downloading pyarrow-19.0.1-cp311-cp311-macosx_12_0_arm64.whl.metadata (3.3 kB)\r\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/vd/anaconda3/lib/python3.11/site-packages (from datasets~=3.2.0) (0.3.6)\r\n",
      "Requirement already satisfied: pandas in /Users/vd/anaconda3/lib/python3.11/site-packages (from datasets~=3.2.0) (2.0.3)\r\n",
      "Collecting requests>=2.32.2 (from datasets~=3.2.0)\r\n",
      "  Obtaining dependency information for requests>=2.32.2 from https://files.pythonhosted.org/packages/f9/9b/335f9764261e915ed497fcdeb11df5dfd6f7bf257d4a6a2a686d80da4d54/requests-2.32.3-py3-none-any.whl.metadata\r\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\r\n",
      "Collecting tqdm>=4.66.3 (from datasets~=3.2.0)\r\n",
      "  Obtaining dependency information for tqdm>=4.66.3 from https://files.pythonhosted.org/packages/d0/30/dc54f88dd4a2b5dc8a0279bdd7270e735851848b762aeb1c1184ed1f6b14/tqdm-4.67.1-py3-none-any.whl.metadata\r\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m57.7/57.7 kB\u001B[0m \u001B[31m4.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: xxhash in /Users/vd/anaconda3/lib/python3.11/site-packages (from datasets~=3.2.0) (2.0.2)\r\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/vd/anaconda3/lib/python3.11/site-packages (from datasets~=3.2.0) (0.70.14)\r\n",
      "Requirement already satisfied: fsspec[http]<=2024.9.0,>=2023.1.0 in /Users/vd/anaconda3/lib/python3.11/site-packages (from datasets~=3.2.0) (2023.4.0)\r\n",
      "Requirement already satisfied: aiohttp in /Users/vd/anaconda3/lib/python3.11/site-packages (from datasets~=3.2.0) (3.8.5)\r\n",
      "Collecting huggingface-hub>=0.23.0 (from datasets~=3.2.0)\r\n",
      "  Obtaining dependency information for huggingface-hub>=0.23.0 from https://files.pythonhosted.org/packages/93/27/1fb384a841e9661faad1c31cbfa62864f59632e876df5d795234da51c395/huggingface_hub-0.30.2-py3-none-any.whl.metadata\r\n",
      "  Downloading huggingface_hub-0.30.2-py3-none-any.whl.metadata (13 kB)\r\n",
      "Requirement already satisfied: packaging in /Users/vd/anaconda3/lib/python3.11/site-packages (from datasets~=3.2.0) (23.1)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/vd/anaconda3/lib/python3.11/site-packages (from datasets~=3.2.0) (6.0)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/vd/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets~=3.2.0) (22.1.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /Users/vd/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets~=3.2.0) (2.0.4)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/vd/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets~=3.2.0) (6.0.2)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/vd/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets~=3.2.0) (4.0.2)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/vd/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets~=3.2.0) (1.8.1)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/vd/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets~=3.2.0) (1.3.3)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/vd/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets~=3.2.0) (1.2.0)\r\n",
      "INFO: pip is looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\r\n",
      "  Obtaining dependency information for huggingface-hub>=0.23.0 from https://files.pythonhosted.org/packages/99/e3/2232d0e726d4d6ea69643b9593d97d0e7e6ea69c2fe9ed5de34d476c1c47/huggingface_hub-0.30.1-py3-none-any.whl.metadata\r\n",
      "  Downloading huggingface_hub-0.30.1-py3-none-any.whl.metadata (13 kB)\r\n",
      "  Obtaining dependency information for huggingface-hub>=0.23.0 from https://files.pythonhosted.org/packages/05/54/ab179a3cbc6cb285b34dfb1686176db4c3e7d9634b23cabf5aff0d00c776/huggingface_hub-0.30.0-py3-none-any.whl.metadata\r\n",
      "  Downloading huggingface_hub-0.30.0-py3-none-any.whl.metadata (13 kB)\r\n",
      "  Obtaining dependency information for huggingface-hub>=0.23.0 from https://files.pythonhosted.org/packages/40/0c/37d380846a2e5c9a3c6a73d26ffbcfdcad5fc3eacf42fdf7cff56f2af634/huggingface_hub-0.29.3-py3-none-any.whl.metadata\r\n",
      "  Downloading huggingface_hub-0.29.3-py3-none-any.whl.metadata (13 kB)\r\n",
      "  Obtaining dependency information for huggingface-hub>=0.23.0 from https://files.pythonhosted.org/packages/13/5f/088ff08dc41808fcd99d9972b9bcfa7e3a35e30e8b0a3155b57938f1611c/huggingface_hub-0.29.2-py3-none-any.whl.metadata\r\n",
      "  Downloading huggingface_hub-0.29.2-py3-none-any.whl.metadata (13 kB)\r\n",
      "  Obtaining dependency information for huggingface-hub>=0.23.0 from https://files.pythonhosted.org/packages/ae/05/75b90de9093de0aadafc868bb2fa7c57651fd8f45384adf39bd77f63980d/huggingface_hub-0.29.1-py3-none-any.whl.metadata\r\n",
      "  Downloading huggingface_hub-0.29.1-py3-none-any.whl.metadata (13 kB)\r\n",
      "  Obtaining dependency information for huggingface-hub>=0.23.0 from https://files.pythonhosted.org/packages/2a/4d/8092df2cb0cafa9fcaf691db851b2fccfe9cad4048e081436bbbdf56e4e1/huggingface_hub-0.29.0-py3-none-any.whl.metadata\r\n",
      "  Downloading huggingface_hub-0.29.0-py3-none-any.whl.metadata (13 kB)\r\n",
      "  Obtaining dependency information for huggingface-hub>=0.23.0 from https://files.pythonhosted.org/packages/ea/da/6c2bea5327b640920267d3bf2c9fc114cfbd0a5de234d81cda80cc9e33c8/huggingface_hub-0.28.1-py3-none-any.whl.metadata\r\n",
      "  Downloading huggingface_hub-0.28.1-py3-none-any.whl.metadata (13 kB)\r\n",
      "INFO: pip is still looking at multiple versions of huggingface-hub to determine which version is compatible with other requirements. This could take a while.\r\n",
      "  Obtaining dependency information for huggingface-hub>=0.23.0 from https://files.pythonhosted.org/packages/cc/ac/07f92291add9f425f40b3fd70a1d0c7117f6e1152599abc2bd7fda5b6abe/huggingface_hub-0.28.0-py3-none-any.whl.metadata\r\n",
      "  Downloading huggingface_hub-0.28.0-py3-none-any.whl.metadata (13 kB)\r\n",
      "  Obtaining dependency information for huggingface-hub>=0.23.0 from https://files.pythonhosted.org/packages/6c/3f/50f6b25fafdcfb1c089187a328c95081abf882309afd86f4053951507cd1/huggingface_hub-0.27.1-py3-none-any.whl.metadata\r\n",
      "  Downloading huggingface_hub-0.27.1-py3-none-any.whl.metadata (13 kB)\r\n",
      "  Obtaining dependency information for huggingface-hub>=0.23.0 from https://files.pythonhosted.org/packages/61/8c/fbdc0a88a622d9fa54e132d7bf3ee03ec602758658a2db5b339a65be2cfe/huggingface_hub-0.27.0-py3-none-any.whl.metadata\r\n",
      "  Downloading huggingface_hub-0.27.0-py3-none-any.whl.metadata (13 kB)\r\n",
      "  Obtaining dependency information for huggingface-hub>=0.23.0 from https://files.pythonhosted.org/packages/44/5a/dc6af87c61f89b23439eb95521e4e99862636cfd538ae12fd36be5483e5f/huggingface_hub-0.26.5-py3-none-any.whl.metadata\r\n",
      "  Downloading huggingface_hub-0.26.5-py3-none-any.whl.metadata (13 kB)\r\n",
      "  Obtaining dependency information for huggingface-hub>=0.23.0 from https://files.pythonhosted.org/packages/95/9b/3068fb3ae0b498eb66960ca5f4d92a81c91458cacd4dc17bfa6d40ce90fb/huggingface_hub-0.26.3-py3-none-any.whl.metadata\r\n",
      "  Downloading huggingface_hub-0.26.3-py3-none-any.whl.metadata (13 kB)\r\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\r\n",
      "  Obtaining dependency information for huggingface-hub>=0.23.0 from https://files.pythonhosted.org/packages/60/bf/cea0b9720c32fa01b0c4ec4b16b9f4ae34ca106b202ebbae9f03ab98cd8f/huggingface_hub-0.26.2-py3-none-any.whl.metadata\r\n",
      "  Downloading huggingface_hub-0.26.2-py3-none-any.whl.metadata (13 kB)\r\n",
      "  Obtaining dependency information for huggingface-hub>=0.23.0 from https://files.pythonhosted.org/packages/d7/4d/017d8d7cff5100092da8ea19139bcb1965bbadcbb5ddd0480e2badc299e8/huggingface_hub-0.26.1-py3-none-any.whl.metadata\r\n",
      "  Downloading huggingface_hub-0.26.1-py3-none-any.whl.metadata (13 kB)\r\n",
      "  Obtaining dependency information for huggingface-hub>=0.23.0 from https://files.pythonhosted.org/packages/85/1f/21e2fb9564b53122105866c26daeb239911b003d1df3867dd113cf5a7694/huggingface_hub-0.26.0-py3-none-any.whl.metadata\r\n",
      "  Downloading huggingface_hub-0.26.0-py3-none-any.whl.metadata (13 kB)\r\n",
      "  Obtaining dependency information for huggingface-hub>=0.23.0 from https://files.pythonhosted.org/packages/64/09/a535946bf2dc88e61341f39dc507530411bb3ea4eac493e5ec833e8f35bd/huggingface_hub-0.25.2-py3-none-any.whl.metadata\r\n",
      "  Downloading huggingface_hub-0.25.2-py3-none-any.whl.metadata (13 kB)\r\n",
      "  Obtaining dependency information for huggingface-hub>=0.23.0 from https://files.pythonhosted.org/packages/5f/f1/15dc793cb109a801346f910a6b350530f2a763a6e83b221725a0bcc1e297/huggingface_hub-0.25.1-py3-none-any.whl.metadata\r\n",
      "  Downloading huggingface_hub-0.25.1-py3-none-any.whl.metadata (13 kB)\r\n",
      "  Obtaining dependency information for huggingface-hub>=0.23.0 from https://files.pythonhosted.org/packages/d5/ce/1f8e61cd63175cc2e79233b954b1c4e85363c788fb3a1fa23c87a25c9b81/huggingface_hub-0.25.0-py3-none-any.whl.metadata\r\n",
      "  Downloading huggingface_hub-0.25.0-py3-none-any.whl.metadata (13 kB)\r\n",
      "  Obtaining dependency information for huggingface-hub>=0.23.0 from https://files.pythonhosted.org/packages/57/28/a0b0dd3cca63908045edc300360d6cd8758d4d86eee3fd2b08f00c5a41c4/huggingface_hub-0.24.7-py3-none-any.whl.metadata\r\n",
      "  Downloading huggingface_hub-0.24.7-py3-none-any.whl.metadata (13 kB)\r\n",
      "  Obtaining dependency information for huggingface-hub>=0.23.0 from https://files.pythonhosted.org/packages/b9/8f/d6718641c14d98a5848c6a24d2376028d292074ffade0702940a4b1dde76/huggingface_hub-0.24.6-py3-none-any.whl.metadata\r\n",
      "  Downloading huggingface_hub-0.24.6-py3-none-any.whl.metadata (13 kB)\r\n",
      "  Obtaining dependency information for huggingface-hub>=0.23.0 from https://files.pythonhosted.org/packages/0b/05/31b21998f68c31e7ffcc27ff08531fb9af5506d765ce8d661fb0036e6918/huggingface_hub-0.24.5-py3-none-any.whl.metadata\r\n",
      "  Downloading huggingface_hub-0.24.5-py3-none-any.whl.metadata (13 kB)\r\n",
      "  Obtaining dependency information for huggingface-hub>=0.23.0 from https://files.pythonhosted.org/packages/24/e6/25bf55a201ee15aed4e87dc4ff1182436986ea6ffd97a941aeacf8d559ab/huggingface_hub-0.24.4-py3-none-any.whl.metadata\r\n",
      "  Downloading huggingface_hub-0.24.4-py3-none-any.whl.metadata (13 kB)\r\n",
      "  Obtaining dependency information for huggingface-hub>=0.23.0 from https://files.pythonhosted.org/packages/0f/36/83c0f0c7a5ec75738241c4c0c066097e4f74729716961db6a2905395015c/huggingface_hub-0.24.3-py3-none-any.whl.metadata\r\n",
      "  Downloading huggingface_hub-0.24.3-py3-none-any.whl.metadata (13 kB)\r\n",
      "  Obtaining dependency information for huggingface-hub>=0.23.0 from https://files.pythonhosted.org/packages/93/14/6a82b1c2eab5a828f7d3d675811660eb68424e8b039191f418a94e8d9726/huggingface_hub-0.24.2-py3-none-any.whl.metadata\r\n",
      "  Downloading huggingface_hub-0.24.2-py3-none-any.whl.metadata (13 kB)\r\n",
      "  Obtaining dependency information for huggingface-hub>=0.23.0 from https://files.pythonhosted.org/packages/96/e6/a1fd9cccd2c08244243aeef71b61cb9b2ba26575d8fd6f7c41edc95e9de0/huggingface_hub-0.24.1-py3-none-any.whl.metadata\r\n",
      "  Downloading huggingface_hub-0.24.1-py3-none-any.whl.metadata (13 kB)\r\n",
      "  Obtaining dependency information for huggingface-hub>=0.23.0 from https://files.pythonhosted.org/packages/57/c0/cf4435f3186655e3bafdca08cd6c794e3866f1f89ed99595504e7240b6a2/huggingface_hub-0.24.0-py3-none-any.whl.metadata\r\n",
      "  Downloading huggingface_hub-0.24.0-py3-none-any.whl.metadata (13 kB)\r\n",
      "  Obtaining dependency information for huggingface-hub>=0.23.0 from https://files.pythonhosted.org/packages/2a/6e/46c71094566fe58663bd661a8ce11c26b25d717a11ed8289f6b94ad72a3b/huggingface_hub-0.23.5-py3-none-any.whl.metadata\r\n",
      "  Downloading huggingface_hub-0.23.5-py3-none-any.whl.metadata (12 kB)\r\n",
      "  Obtaining dependency information for huggingface-hub>=0.23.0 from https://files.pythonhosted.org/packages/69/d6/73f9d1b7c4da5f0544bc17680d0fa9932445423b90cd38e1ee77d001a4f5/huggingface_hub-0.23.4-py3-none-any.whl.metadata\r\n",
      "  Downloading huggingface_hub-0.23.4-py3-none-any.whl.metadata (12 kB)\r\n",
      "  Obtaining dependency information for huggingface-hub>=0.23.0 from https://files.pythonhosted.org/packages/66/e8/bbbad5c7b49c68def42830f96c606e693bfa935a886740a363f04cb84e44/huggingface_hub-0.23.3-py3-none-any.whl.metadata\r\n",
      "  Downloading huggingface_hub-0.23.3-py3-none-any.whl.metadata (12 kB)\r\n",
      "  Obtaining dependency information for huggingface-hub>=0.23.0 from https://files.pythonhosted.org/packages/78/71/6ce4136149cb42b98599d49c39b3a39dd6858b5f9307490998c40e26a51e/huggingface_hub-0.23.2-py3-none-any.whl.metadata\r\n",
      "  Downloading huggingface_hub-0.23.2-py3-none-any.whl.metadata (12 kB)\r\n",
      "  Obtaining dependency information for huggingface-hub>=0.23.0 from https://files.pythonhosted.org/packages/92/27/1a30d8082ef3c8615ae198b9d451fafffdab815b96727ec3c06befc27ebe/huggingface_hub-0.23.1-py3-none-any.whl.metadata\r\n",
      "  Downloading huggingface_hub-0.23.1-py3-none-any.whl.metadata (12 kB)\r\n",
      "  Obtaining dependency information for huggingface-hub>=0.23.0 from https://files.pythonhosted.org/packages/21/2b/516f82c5ba9beb184b24c11976be2ad5e80fb7fe6b2796c887087144445e/huggingface_hub-0.23.0-py3-none-any.whl.metadata\r\n",
      "  Downloading huggingface_hub-0.23.0-py3-none-any.whl.metadata (12 kB)\r\n",
      "Collecting fsspec[http]<=2024.9.0,>=2023.1.0 (from datasets~=3.2.0)\r\n",
      "  Obtaining dependency information for fsspec[http]<=2024.9.0,>=2023.1.0 from https://files.pythonhosted.org/packages/1d/a0/6aaea0c2fbea2f89bfd5db25fb1e3481896a423002ebe4e55288907a97a3/fsspec-2024.9.0-py3-none-any.whl.metadata\r\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/vd/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.23.0->datasets~=3.2.0) (4.12.1)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/vd/anaconda3/lib/python3.11/site-packages (from requests>=2.32.2->datasets~=3.2.0) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/vd/anaconda3/lib/python3.11/site-packages (from requests>=2.32.2->datasets~=3.2.0) (1.26.16)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/vd/anaconda3/lib/python3.11/site-packages (from requests>=2.32.2->datasets~=3.2.0) (2024.8.30)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/vd/anaconda3/lib/python3.11/site-packages (from pandas->datasets~=3.2.0) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/vd/anaconda3/lib/python3.11/site-packages (from pandas->datasets~=3.2.0) (2023.3.post1)\r\n",
      "Requirement already satisfied: tzdata>=2022.1 in /Users/vd/anaconda3/lib/python3.11/site-packages (from pandas->datasets~=3.2.0) (2023.3)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/vd/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets~=3.2.0) (1.16.0)\r\n",
      "Downloading datasets-3.2.0-py3-none-any.whl (480 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m480.6/480.6 kB\u001B[0m \u001B[31m18.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading huggingface_hub-0.30.2-py3-none-any.whl (481 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m481.4/481.4 kB\u001B[0m \u001B[31m32.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m179.3/179.3 kB\u001B[0m \u001B[31m21.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hDownloading pyarrow-19.0.1-cp311-cp311-macosx_12_0_arm64.whl (30.7 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m30.7/30.7 MB\u001B[0m \u001B[31m12.0 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hUsing cached requests-2.32.3-py3-none-any.whl (64 kB)\r\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m78.5/78.5 kB\u001B[0m \u001B[31m966.3 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: tqdm, requests, pyarrow, fsspec, huggingface-hub, datasets\r\n",
      "  Attempting uninstall: tqdm\r\n",
      "    Found existing installation: tqdm 4.65.0\r\n",
      "    Uninstalling tqdm-4.65.0:\r\n",
      "      Successfully uninstalled tqdm-4.65.0\r\n",
      "  Attempting uninstall: requests\r\n",
      "    Found existing installation: requests 2.31.0\r\n",
      "    Uninstalling requests-2.31.0:\r\n",
      "      Successfully uninstalled requests-2.31.0\r\n",
      "  Attempting uninstall: pyarrow\r\n",
      "    Found existing installation: pyarrow 11.0.0\r\n",
      "    Uninstalling pyarrow-11.0.0:\r\n",
      "      Successfully uninstalled pyarrow-11.0.0\r\n",
      "  Attempting uninstall: fsspec\r\n",
      "    Found existing installation: fsspec 2023.4.0\r\n",
      "    Uninstalling fsspec-2023.4.0:\r\n",
      "      Successfully uninstalled fsspec-2023.4.0\r\n",
      "  Attempting uninstall: huggingface-hub\r\n",
      "    Found existing installation: huggingface-hub 0.15.1\r\n",
      "    Uninstalling huggingface-hub-0.15.1:\r\n",
      "      Successfully uninstalled huggingface-hub-0.15.1\r\n",
      "  Attempting uninstall: datasets\r\n",
      "    Found existing installation: datasets 2.12.0\r\n",
      "    Uninstalling datasets-2.12.0:\r\n",
      "      Successfully uninstalled datasets-2.12.0\r\n",
      "\u001B[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "conda-repo-cli 1.0.75 requires requests_mock, which is not installed.\r\n",
      "conda-repo-cli 1.0.75 requires clyent==1.2.1, but you have clyent 1.2.2 which is incompatible.\r\n",
      "conda-repo-cli 1.0.75 requires PyYAML==6.0.1, but you have pyyaml 6.0 which is incompatible.\r\n",
      "conda-repo-cli 1.0.75 requires requests==2.31.0, but you have requests 2.32.3 which is incompatible.\r\n",
      "s3fs 2023.4.0 requires fsspec==2023.4.0, but you have fsspec 2024.9.0 which is incompatible.\u001B[0m\u001B[31m\r\n",
      "\u001B[0mSuccessfully installed datasets-3.2.0 fsspec-2024.9.0 huggingface-hub-0.30.2 pyarrow-19.0.1 requests-2.32.3 tqdm-4.67.1\r\n",
      "Requirement already satisfied: matplotlib in /Users/vd/anaconda3/lib/python3.11/site-packages (3.7.2)\r\n",
      "Collecting matplotlib\r\n",
      "  Obtaining dependency information for matplotlib from https://files.pythonhosted.org/packages/0a/e4/300b08e3e08f9c98b0d5635f42edabf2f7a1d634e64cb0318a71a44ff720/matplotlib-3.10.1-cp311-cp311-macosx_11_0_arm64.whl.metadata\r\n",
      "  Downloading matplotlib-3.10.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (11 kB)\r\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/vd/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.0.5)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/vd/anaconda3/lib/python3.11/site-packages (from matplotlib) (0.11.0)\r\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/vd/anaconda3/lib/python3.11/site-packages (from matplotlib) (4.25.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/vd/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.4.4)\r\n",
      "Requirement already satisfied: numpy>=1.23 in /Users/vd/anaconda3/lib/python3.11/site-packages (from matplotlib) (1.24.3)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/vd/anaconda3/lib/python3.11/site-packages (from matplotlib) (23.1)\r\n",
      "Requirement already satisfied: pillow>=8 in /Users/vd/anaconda3/lib/python3.11/site-packages (from matplotlib) (10.2.0)\r\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/vd/anaconda3/lib/python3.11/site-packages (from matplotlib) (3.0.9)\r\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/vd/anaconda3/lib/python3.11/site-packages (from matplotlib) (2.8.2)\r\n",
      "Requirement already satisfied: six>=1.5 in /Users/vd/anaconda3/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\r\n",
      "Downloading matplotlib-3.10.1-cp311-cp311-macosx_11_0_arm64.whl (8.0 MB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m8.0/8.0 MB\u001B[0m \u001B[31m13.3 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m00:01\u001B[0m00:01\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: matplotlib\r\n",
      "  Attempting uninstall: matplotlib\r\n",
      "    Found existing installation: matplotlib 3.7.2\r\n",
      "    Uninstalling matplotlib-3.7.2:\r\n",
      "      Successfully uninstalled matplotlib-3.7.2\r\n",
      "Successfully installed matplotlib-3.10.1\r\n"
     ]
    }
   ],
   "source": [
    "! pip install -U datasets~=3.2.0\n",
    "! python -m pip install -U matplotlib"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2025-04-10T15:53:47.533235Z",
     "start_time": "2025-04-10T15:53:31.959257Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import tqdm"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2025-04-10T15:54:18.603882Z",
     "start_time": "2025-04-10T15:54:15.580702Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 0. Intro to regular expressions\n",
    "\n",
    "In this introduction section, you can practice the use of regular expressions in python. You can find the documentation here: [https://docs.python.org/3/library/re.html](https://docs.python.org/3/library/re.html). The main functions of the re module are:\n",
    "- `re.search()` - searches for a pattern in a string, returns the first match,\n",
    "- `re.findall()` - similar to `search()`, but returns a list of all matches,\n",
    "- `re.sub()` - replaces the matches with a string.\n",
    "\n",
    "All above functions accept the regular expression pattern as their argument. The patterns are strings that represent the rules for matching the text. In python they start with `r` character, e.g. `r'\\d'` is a pattern that matches a digit.\n",
    "\n",
    "Let us start with a simple example. We will search for the word \"world\" in the string \"Hello, world!\"."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(7, 12), match='world'>\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, world!\"\n",
    "pattern = r'world'\n",
    "match = re.search(pattern, text)\n",
    "print(match)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2025-04-10T15:54:20.581277Z",
     "start_time": "2025-04-10T15:54:20.534572Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The `search()` function returns a match object that tells us where the match was found (`span` argument) and the exact part of the string that matched the pattern (`group` argument).\n",
    "\n",
    "Below you can find the examples from the lecture."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Disjunctions\n",
    "pattern = r'[wW]oodchuck' # matches both \"woodchuck\" and \"Woodchuck\"\n",
    "pattern = r'[1234567890]' # matches any digit\n",
    "pattern = r'[0-9]' # matches any digit\n",
    "pattern = r'[A-Z]' # matches any uppercase letter\n",
    "pattern = r'[a-z]' # matches any lowercase letter\n",
    "pattern = r'[A-Za-z]' # matches any letter\n",
    "\n",
    "# Disjunctions with pipe |\n",
    "pattern = r'groundhog|Woodchuck' # matches both \"woodchuck\" and \"Woodchuck\"\n",
    "\n",
    "# Negation (only when in [])\n",
    "pattern = r'[^0-9]' # matches any character that is not a digit\n",
    "pattern = r'[^Ss]' # matches any character that is not 'S' or 's'\n",
    "pattern = r'a^b' # matches the string \"a^b\"\n",
    "\n",
    "# Quantifiers (+, *, ?, .)\n",
    "pattern = r'baa+' # matches \"ba\" followed by one or more \"a\" (e.g. \"baa\", \"baaa\", \"baaaa\", ...)\n",
    "pattern = r'oo*h' # matches \"o\" followed by zero or more \"o\" and then \"h\" (e.g. \"oh\", \"ooh\", \"oooh\", ...)\n",
    "pattern = r'colou?r' # matches \"color\" and \"colour\"\n",
    "pattern = r'beg.n' # matches \"begun\", \"begin\", \"begnn\", ...\n",
    "\n",
    "# Anchors (^, $)\n",
    "pattern = r'^Hello' # matches \"Hello\" at the beginning of the string\n",
    "pattern = r'world!$' # matches \"world!\" at the end of the string"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "ExecuteTime": {
     "end_time": "2025-04-10T15:54:23.525762Z",
     "start_time": "2025-04-10T15:54:23.521149Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Huggingface datasets\n",
    "\n",
    "For this lab, we will use the **Hugging Face Datasets** library ([Hugging Face Datasets](https://huggingface.co/datasets)), which provides an extensive collection of ready-to-use NLP datasets. The library is designed to be lightweight, efficient, and compatible with popular deep learning frameworks such as PyTorch and TensorFlow.  \n",
    "\n",
    "You can find the full documentation and tutorials here:  \n",
    "📌 [Hugging Face Datasets Documentation](https://huggingface.co/docs/datasets/en/index)  \n",
    "\n",
    "### **Why use Hugging Face Datasets?**  \n",
    "- **Easy Access:** Load datasets with a single command without manual downloads.  \n",
    "- **Standardized Format:** Datasets come in a unified structure, making them easy to preprocess and integrate into ML pipelines.  \n",
    "- **Large Collection:** Provides datasets for a wide range of NLP tasks, including classification, translation, summarization, and more.  \n",
    "- **Seamless Integration:** Works with `transformers` and `sklearn` for preprocessing and model training.  \n",
    "\n",
    "### **Dataset for this lab: TweetEval - Emoji Subset**  \n",
    "\n",
    "In this lab, we will work with the **TweetEval** dataset, specifically the **emoji** subset. The TweetEval dataset is a benchmark for evaluating NLP models on Twitter-related tasks, covering tasks such as sentiment analysis, hate speech detection, and irony detection.  \n",
    "\n",
    "For tokenization, we will focus only on the **text** (the content of the tweets), but we will also examine the **labels** to understand the dataset structure.  \n",
    "\n",
    "🔗 The dataset description and details are available in its dataset card: [**TweetEval Dataset**](https://huggingface.co/datasets/cardiffnlp/tweet_eval) \n",
    "\n",
    "💡 Exploring More Datasets\n",
    "Hugging Face provides a vast selection of datasets across different NLP tasks. You can browse and explore more at:\n",
    "🔗 [Hugging Face Datasets Collection](https://huggingface.co/datasets)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tweet_ds = datasets.load_dataset('tweet_eval', 'emoji')\n",
    "print(tweet_ds)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The loaded dataset contains three subsets (“train”, “validation”, and “test”). Each consists of two columns: “text” and “label”. Label is an integer from 0 to 19 representing an emoji. See the dataset's card for more information. We can access the elements of the dataset like so:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(tweet_ds['train'][i])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can easily cast the dataset to the pandas DataFrame."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tweet_train_df = pd.DataFrame(tweet_ds['train'])\n",
    "print(tweet_train_df)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can plot the distribution of the labels in the training subset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tweet_train_df.groupby('label').count().plot.bar()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataset's filter function\n",
    "We can filter the examples using ```filter()``` method. See this link for more details https://huggingface.co/docs/datasets/en/use_dataset. Here is an example of filtering the short tweets (less than 20 characters) from the ```train``` subset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "short_tweets = tweet_ds['train'].filter(lambda example: len(example['text']) < 20)\n",
    "print(short_tweets)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(short_tweets[i])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataset's map function\n",
    "Datasets library contains a very useful method map. It expects a function that will receive an example from the dataset. This function will be applied to all entries. We will calculate the length of the text (in characters) in each example."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_text_length(example):\n",
    "    example['text_length'] = len(example['text'])\n",
    "    return example"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tweet_ds = tweet_ds.map(calculate_text_length)\n",
    "print(tweet_ds)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can plot the histogram of the text lengths."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.DataFrame(tweet_ds['train']).groupby('text_length')['text_length'].count().plot.hist(bins=50)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a name=\"e1\"></a>\n",
    "\n",
    "### Exercise 1: Questions about the datasets\n",
    "1. (1p) What is the size of the training, test and validation datasets?\n",
    "2. (1p) What are the top 5 most frequent emojis in the validation dataset?\n",
    "3. (1p) Compare the distributions of labels (emojis) between training and validation datasets.\n",
    "4. (1p) How many examples with the \"fire\" emoji are in the training dataset?\n",
    "5. (1p) What is the average length (in characters) of the tweets in the training dataset?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "You can add cells here to answer the questions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Tokenization\n",
    "\n",
    "In this section we will preprocess the dataset by cleaning and tokenizing the entries.\n",
    "Datasets library contains a very useful method map. It expects a function that will receive an example from the dataset. This function will be applied to all entries."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.1 Cleaning the text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a name=\"e2\"></a>\n",
    "### Exercise 2: Write the text cleaning function\n",
    "\n",
    "Include at least the following steps:\n",
    "- (1p) remove comma between numbers, i.e. 15,000 -> 15000\n",
    "- (1p) remove multiple spaces\n",
    "- (1p) space out the punctuation (i.e. \"hello, world.\" -> \"hello , world .\")\n",
    "- (3x1p) three more cleaning steps of your choice"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def clean(example):\n",
    "    \"\"\"\n",
    "    Cleans the example from the Dataset\n",
    "    Args:\n",
    "        example: an example from the Dataset\n",
    "\n",
    "    Returns: update example containing 'clean' column\n",
    "\n",
    "    \"\"\"\n",
    "    text = example['text']\n",
    "\n",
    "    # Empty text\n",
    "    if text == '':\n",
    "        example['clean'] = ''\n",
    "        return example\n",
    "\n",
    "    # 'text' from the example can be of type numpy.str_, let's convert it to a python str\n",
    "    text = str(text)\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "    # remove comma between numbers\n",
    "\n",
    "    # remove multiple spaces\n",
    "\n",
    "    # space out the punctuation\n",
    "\n",
    "    # three more cleaning steps of your choice\n",
    "\n",
    "    ### YOUR CODE ENDS HERE\n",
    "\n",
    "    # Update the example with the cleaned text\n",
    "    example['clean'] = text.strip()\n",
    "    return example"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is an example of applying the ```clean()``` function you just wrote to a single entry of the dataset. The function added a 'clean' field to the example."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Original tweet item:')\n",
    "print(tweet_ds['train'][1]['text'])\n",
    "print('Cleaned tweet item:')\n",
    "print(clean(tweet_ds['train'][1])['clean'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's finally use the ```map()``` method and apply your `clean()` function to all entries of the dataset. You can see that the ```clean``` column has been added to each split."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Below, we will apply your function to all entries in the dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tweet_ds = tweet_ds.map(clean)\n",
    "print(tweet_ds)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.2 Build vocabulary\n",
    "\n",
    "In the previous section, we implemented the cleaning of the dataset. Now, we will tokenize the text splitting it by spaces. We will build a vocabulary based on the cleaned text of the `train` split. We will investigate some properties of corpora (e.g. Zipf's law).\n",
    "\n",
    "The function below builds a vocabulary from the dataset. It counts the occurrences of the words in the dataset using the Counter class. Check the documentation here [collections.Counter](https://docs.python.org/3/library/collections.html#collections.Counter)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 3: Build the vocabulary\n",
    "(5p) Fill in the function below to build the vocabulary from the dataset. The function should return a `Counter` object with the words and their frequencies. The variable named `vocab` is already initialized as an empty `Counter` object."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_vocab_counter(dataset):\n",
    "    \"\"\"\n",
    "    Builds a vocabulary from the dataset\n",
    "    Args:\n",
    "        dataset: a dataset\n",
    "\n",
    "    Returns: a vocabulary\n",
    "\n",
    "    \"\"\"\n",
    "    vocab = Counter()\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ### YOUR CODE ENDS HERE\n",
    "    return vocab"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "vocab_counter = build_vocab_counter(tweet_ds['train'])\n",
    "print('Size of the vocabulary:', len(vocab_counter))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Because we created a counter, we can easily check the most and least common words in the vocabulary. Do the most common words make sense? How about the least common ones?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print('Most common:')\n",
    "print(vocab_counter.most_common(10))\n",
    "print('Least common:')\n",
    "print(vocab_counter.most_common()[-10:])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also plot the counts of the words. You can check the [Power law](https://en.wikipedia.org/wiki/Power_law) if you are more interested."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.loglog([val for word, val in vocab_counter.most_common()])\n",
    "plt.xlabel('rank')\n",
    "plt.ylabel('count')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The plot shows that the distribution of the words in the vocabulary follows the Zipf's law. The most frequent word occurs approximately twice as often as the second most frequent word, three times as often as the third most frequent word, etc.\n",
    "\n",
    "We can also filter the vocabulary by the frequency of the words. We will only consider the most frequent words and mark the rest as the `<unk>` token. Here we set the maximum vocabulary size to 10,000. But in the later steps, you will experiment with different sizes."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max_vocab_size = 10000\n",
    "vocab = vocab_counter.most_common(max_vocab_size)\n",
    "# cast to list of words\n",
    "vocab = [word for word, _ in vocab]\n",
    "print(len(vocab))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 4: Frequency of pairs of words (bigrams)\n",
    "Calculate the frequency of (neighbouring) pairs of words in the training dataset.\n",
    "- (5p) List the most and least common pairs. Do the most common pairs make sense?\n",
    "- (2p) How many pairs occur only once in the dataset?\n",
    "- (5p) Plot the distribution of the pair frequencies."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2.3 Tokenize the dataset\n",
    "The function below tokenizes the cleaned text (```example['clean']```) by splitting it on spaces. It replaces the words that are not in the vocabulary with the `<unk>` token."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 5: Tokenize the dataset\n",
    "\n",
    "(5p) Fill in the function below to tokenize the dataset. The function will be applied to the dataset through the `map()` method, so it returns the updated example. Your task is to split the text by spaces and replace the words that are not in the vocabulary with the `<unk>` token."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def tokenize(example, vocab, unknown_token='<unk>'):\n",
    "    \"\"\"\n",
    "    Tokenizes the example from the Dataset\n",
    "    Args:\n",
    "        example: an example from the Dataset\n",
    "        vocab: a vocabulary as a list of words\n",
    "        unknown_token: a token to replace the words that are not in the vocabulary\n",
    "    Returns: update example containing 'tokens' column\n",
    "\n",
    "    \"\"\"\n",
    "    text = example['clean']\n",
    "    tokens = None # list of tokens, your code should fill this variable\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    ### YOUR CODE ENDS HERE\n",
    "\n",
    "    example['tokens'] = tokens\n",
    "    return example"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tweet_ds = tweet_ds.map(tokenize, fn_kwargs={'vocab': vocab})\n",
    "print(tweet_ds)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us examine several entries from the dataset. We can see that the `tokens` column has been added to each example."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print('Original tweet:')\n",
    "    print(tweet_ds['train'][i]['text'])\n",
    "    print('Tokenized tweet:')\n",
    "    print(tweet_ds['train'][i]['tokens'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Make sure that the tokenization works as you intended. If not, revisit the cleaning and tokenization functions."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 6: Questions about the tokenization\n",
    "1. (3p) How many unknown tokens are in the validation dataset after tokenization?\n",
    "2. (3p) What is the distribution of the number of tokens in the training dataset?\n",
    "3. (4p) How the number of tokens corresponds to the number of characters in our dataset?\n",
    "4. (4p) How the size of the vocabulary (```max_vocab_size```) affects the number of unknown tokens?\n",
    "5. (4p) How does the size of the vocabulary affect the number of tokens in the dataset?\n",
    "6. (4p) Think about the advantages and disadvantages of the tokenization method we used. What are the cases when it will not work well?\n",
    "\n",
    "For answering these questions make sure to include a proper mix of numbers/plots/tables etc. and comments."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3. Byte Pair Encoding\n",
    "\n",
    "In this section, you will build the Byte Pair Encoding (BPE) tokenizer. BPE is an algorithm that replaces the most frequent pair of tokens (initially characters) with a new token. The algorithm is configured by the number of merges that are performed. You can find the paper here [Neural Machine Translation of Rare Words with Subword Units](https://arxiv.org/abs/1508.07909)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.1 Finding the initial set of characters\n",
    "BPE algorithm starts with the set of characters that occur in the dataset. We will build a character counter from the dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 7: Counting the characters\n",
    "\n",
    "(5p) In this exercise, we build a counter with the frequencies of all characters in the dataset. Iterate over the dataset and count the characters in the `clean` column. The function returns a `Counter` object with the characters and their frequencies."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def build_character_counter(dataset):\n",
    "    \"\"\"\n",
    "    Builds a character counter from the dataset\n",
    "    Args:\n",
    "        dataset: a dataset\n",
    "\n",
    "    Returns: a character counter\n",
    "\n",
    "    \"\"\"\n",
    "    char_counter = Counter()\n",
    "\n",
    "    ### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "    ### YOUR CODE ENDS HERE\n",
    "\n",
    "    return char_counter"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The next cell applies the function to the training dataset and prints the size of the vocabulary and the most common characters."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "char_counter = build_character_counter(tweet_ds['train'])\n",
    "print(len(char_counter))\n",
    "print(char_counter.most_common(100))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will filter the characters that occur less than 10 times in the dataset. We will also replace the space character with the `__` token. This is necessary because we want to preserve the spaces between the words in the tokenization process."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bpe_init_vocab = sorted([char for char, _ in char_counter.most_common() if char_counter[char] >= 10])\n",
    "bpe_init_vocab[bpe_init_vocab.index(' ')] = '__'\n",
    "print(bpe_init_vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.2 Training the BPE tokenizer\n",
    "\n",
    "In this section, we will implement the BPE algorithm. We will start by initializing the BPE corpus. The corpus is a list of words from the dataset with their frequency. This makes it easier to find the most frequent pairs of neighbouring tokens (or characters in the beginning). Each word is split into characters and the space (the ```__``` token) is added at the end of each word."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def init_bpe_corpus(dataset):\n",
    "    \"\"\"\n",
    "    Initializes the BPE corpus\n",
    "    Args:\n",
    "        dataset: a dataset\n",
    "\n",
    "    Returns: a BPE corpus\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    corpus = Counter()\n",
    "    for example in dataset:\n",
    "        words = example['clean'].split()\n",
    "        words = [' '.join(list(word)) + ' __' for word in words]\n",
    "        corpus.update(words)\n",
    "    return corpus"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bpe_corpus = init_bpe_corpus(tweet_ds['train'])\n",
    "print(len(bpe_corpus))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can check the most common words in the corpus along with their frequencies."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bpe_corpus.most_common(30)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our BPE implementation will consist of the following steps:\n",
    "1. Calculate the frequency statistics of adjacent symbol pairs in the corpus.\n",
    "2. Find the most frequent pair.\n",
    "3. Merge the most frequent pair.\n",
    "4. Repeat until the specified number of merges is reached.\n",
    "\n",
    "The following function calculates the frequency statistics of adjacent symbol pairs in the corpus."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 8: Calculate the frequency statistics of adjacent symbol pairs\n",
    "(5p) Fill in the function below to calculate the frequency statistics of adjacent symbol pairs in the corpus. The function returns a Counter object with the counts of adjacent token pairs. The pairs are represented as tuples of two tokens (e.g., `('cali', 'for')`)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_bpe_corpus_stats(corpus):\n",
    "    \"\"\"\n",
    "    Calculates the frequency statistics of adjacent symbol pairs in the corpus.\n",
    "    Args:\n",
    "        corpus: a BPE corpus as a Counter object with words split by space into tokens (initially characters)\n",
    "\n",
    "    Returns: a Counter object with the frequency statistics of adjacent symbol pairs\n",
    "    \"\"\"\n",
    "    stats = Counter()\n",
    "\n",
    "    for word, freq in corpus.items():\n",
    "\n",
    "        ### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ### YOUR CODE ENDS HERE\n",
    "\n",
    "    return stats"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can check the most common pairs of characters in the initial corpus."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "stats = calculate_bpe_corpus_stats(bpe_corpus)\n",
    "print(stats.most_common(10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we will implement the function that merges the most frequent pair of symbols in the corpus. The function takes the corpus and the most frequent pair of symbols as input and returns the updated corpus."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def merge_corpus(corpus, pair):\n",
    "    \"\"\"\n",
    "    Merges the most frequent pair of symbols in the corpus.\n",
    "    Args:\n",
    "        corpus (dict): Keys are words as space-separated symbols (e.g., \"l o w\"),\n",
    "                       and values are the frequency counts.\n",
    "        pair (tuple): A pair of symbols to merge.\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated corpus after merging the pair of symbols.\n",
    "    \"\"\"\n",
    "    new_corpus = Counter()\n",
    "    bigram = \" \".join(pair)\n",
    "    replacement = \"\".join(pair)\n",
    "    for word, freq in corpus.items():\n",
    "        new_word = word.replace(bigram, replacement)\n",
    "        new_corpus[new_word] = freq\n",
    "    return new_corpus"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The last step is to implement the BPE algorithm. The function takes the initial vocabulary, the corpus, and the number of merges as input. It returns the updated vocabulary, corpus, and the list of merges.\n",
    "Returning the list of merges is useful for the tokenization process - it makes it faster to tokenize the text. It contains the tuples of the two tokens that were merged. For example, ('to', 'day__') will merge the tokens 'to' and 'day__' into the 'today__' token."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 9: BPE algorithm\n",
    "\n",
    "(10p) Implement the BPE algorithm in the following function. The function should return the updated vocabulary, corpus, and the list of merges. The function should perform the specified number of merges. The vocabulary is a list of tokens, the corpus is a Counter object with the words split by space into tokens, and the merges is a list of tuples with the merged tokens.\n",
    "\n",
    "You should use the functions you implemented earlier in this section (```calculate_bpe_corpus_stats()```, ```merge_corpus()```)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def bpe(vocab, corpus, num_merges):\n",
    "    \"\"\"\n",
    "    Applies the BPE algorithm to the corpus. Merges the most frequent adjacent symbol pairs. The function performs the specified number of merges.\n",
    "\n",
    "    Args:\n",
    "        vocab (list): A list of tokens representing the BPE vocabulary.\n",
    "        corpus (Counter): A Counter object with words split by space into tokens.\n",
    "        num_merges (int): The number of merges to perform.\n",
    "\n",
    "    Returns:\n",
    "        list: Updated vocabulary.\n",
    "        Counter: Updated corpus.\n",
    "        list: List of merges.\n",
    "    \"\"\"\n",
    "    vocab = vocab.copy()\n",
    "    corpus = corpus.copy()\n",
    "    merges = []\n",
    "\n",
    "    for i in tqdm.tqdm(range(num_merges)):\n",
    "        ### YOUR CODE HERE\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ### YOUR CODE ENDS HERE\n",
    "    return vocab, corpus, merges"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following cell applies the BPE algorithm to the initial vocabulary and corpus. We will perform 100 merges at first, but you will experiment with different numbers."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "bpe_vocab, updated_bpe_corpus, bpe_merges = bpe(bpe_init_vocab, bpe_corpus, num_merges=100)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can check the size of the BPE vocabulary and the most common tokens."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(len(bpe_vocab))\n",
    "print(bpe_vocab[:150])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also check the most common merges."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(bpe_merges[:150])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3.3 Tokenizing the text using BPE\n",
    "\n",
    "With the tokenizer trained we can now tokenize the text using the BPE vocabulary. We will first build a function that tokenizes any text using our BPE tokenizer (vocabulary and merges). Next we will apply it to our dataset.\n",
    "\n",
    "The following function tokenizes the text using the BPE vocabulary. It replaces the most frequent pairs of tokens with the new token. The function also replaces the tokens that are not in the vocabulary with the `<unk>` token."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def apply_bpe_tokenization(text, vocab, merges, unk_token='<unk>'):\n",
    "    \"\"\"\n",
    "    Tokenizes the text using BPE vocabulary, preserving spaces as '__'.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to be tokenized.\n",
    "        vocab (set): A set containing the BPE vocabulary tokens.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tokens representing the input text.\n",
    "    \"\"\"\n",
    "\n",
    "    words = re.split(r'\\s', text)\n",
    "    words = [' ' + ' '.join(list(word)) + (' __ ' if i < len(words) - 1 else ' ') for i, word in enumerate(words)]\n",
    "\n",
    "    bpe_tokens = []\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        for merge in merges:\n",
    "            word = word.replace(' ' + ' '.join(merge) + ' ', ' ' + ''.join(merge) + ' ')\n",
    "        bpe_tokens.extend(word.split())\n",
    "\n",
    "    for i, token in enumerate(bpe_tokens):\n",
    "        if token not in vocab:\n",
    "            bpe_tokens[i] = unk_token\n",
    "    return bpe_tokens\n",
    "\n",
    "\n",
    "# A test example with a special character. Is the character tokenized correctly as <unk> token?\n",
    "print(apply_bpe_tokenization(tweet_ds['train'][0]['clean'] + ' 🇺', bpe_vocab, bpe_merges))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The function below will apply our BPE tokenizer to the dataset. It will add a new column `bpe_tokens` to each example."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def tokenize_bpe(example, vocab, merges, unk_token='<unk>'):\n",
    "    \"\"\"\n",
    "    Tokenizes the example from the Dataset using BPE\n",
    "    Args:\n",
    "        example: an example from the Dataset\n",
    "        vocab: a BPE vocabulary\n",
    "\n",
    "    Returns: update example containing 'bpe_tokens' column\n",
    "\n",
    "    \"\"\"\n",
    "    text = example['clean']\n",
    "    bpe_tokens = apply_bpe_tokenization(text, vocab, merges, unk_token)\n",
    "    example['bpe_tokens'] = bpe_tokens\n",
    "    return example\n",
    "\n",
    "tweet_ds = tweet_ds.map(tokenize_bpe, fn_kwargs={'vocab': bpe_vocab, 'merges': bpe_merges})\n",
    "print(tweet_ds)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will inspect the both tokenizations of several examples from the ```validation``` subset. Try to find the ```<unk>``` tokens in the printed examples."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print('Original tweet:')\n",
    "    print(tweet_ds['validation'][i]['text'])\n",
    "    print('Word tokenization:')\n",
    "    print(tweet_ds['validation'][i]['tokens'])\n",
    "    print('BPE tokenization:')\n",
    "    print(tweet_ds['validation'][i]['bpe_tokens'])\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Exercise 10: Comparing tokenizers\n",
    "\n",
    "Train the BPE tokenizer with different number of merges. Compare the tokenization results with the word tokenization.\n",
    "1. (5p) What are the differences?\n",
    "2. (5p) Compare the number of tokens created by your tokenizers.\n",
    "3. (5p) Calculate the number of `<unk>` tokens in the validation dataset for each tokenizer.\n",
    "4. (5p) Compare the average length in tokens between different tokenizers.\n",
    "5. (5p) What are the advantages and disadvantages of the BPE tokenizer?"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For answering these questions make sure to include a proper mix of numbers/plots/tables etc. and comments."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
